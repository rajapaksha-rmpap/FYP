{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the `GradientTape` and verify whether the existing DDPG model has some error in gradient calculation and weight updates of the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== REPLAY BUFFER ===============================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, state_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "\n",
    "        self.state_memory     = np.zeros((self.mem_size, *state_shape))\n",
    "        self.action_memory    = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory    = np.zeros(self.mem_size)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *state_shape))\n",
    "        self.terminal_memory  = np.zeros(self.mem_size, dtype=np.bool) # using np.bool is really useful when pytorch is used.\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        index = self.mem_cntr % self.mem_size # implement a queue\n",
    "\n",
    "        self.state_memory[index]     = state\n",
    "        self.action_memory[index]    = action\n",
    "        self.reward_memory[index]    = reward\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.terminal_memory[index]  = done # problematic !!!\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False) # replace = False -> in a single batch, no element gets sampled more than once. \n",
    "\n",
    "        states     = self.state_memory[batch]\n",
    "        actions    = self.action_memory[batch]\n",
    "        rewards    = self.reward_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        dones      = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, new_states, dones\n",
    "\n",
    "\n",
    "\n",
    "# =============================== CRITIC NETWORK ===============================\n",
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            name=\"critic\", # model name (required by tf.keras.Model)\n",
    "            fc1_dims=512,\n",
    "            fc2_dims=512,\n",
    "            chkpt_dir='tmp/ddpg/'\n",
    "    ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.model_name = name # do not use 'self.model'; it is a reserved variable name in tf\n",
    "        self.checkpoint_dir  = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name, '_ddpg.h5') \n",
    "        # extensions for saving keras models: legacy '.h5' -> TF 1.X, '.tf' -> TF 2.X\n",
    "\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "\n",
    "        # define network layers \n",
    "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
    "        self.q   = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        temp1 = self.fc1(tf.concat([state, action], axis=1)) # axis 0 -> batch dimension\n",
    "        # ######################## PROBLEM ########################\n",
    "        # according to the paper, actions were not included until the 2nd hidden layer of Q\n",
    "        temp2 = self.fc2(temp1)\n",
    "        q_value = self.q(temp2)\n",
    "\n",
    "        return q_value\n",
    "\n",
    "# ================================ ACTOR NETWORK ===============================\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            name=\"actor\", # model name (required by tf.keras.Model)\n",
    "            n_actions=2, # action shape (dimenisonality of action space)\n",
    "            fc1_dims=512,\n",
    "            fc2_dims=512,\n",
    "            chkpt_dir='tmp/ddpg/'\n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.model_name = name # do not use 'self.model'; it is a reserved variable name in tf\n",
    "        self.checkpoint_dir  = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name, '_ddpg.h5') \n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "\n",
    "        # define network layers\n",
    "        self.fc1 = Dense(self.fc1_dims,  activation='relu')\n",
    "        self.fc2 = Dense(self.fc2_dims,  activation='relu')\n",
    "        self.mu  = Dense(self.n_actions, activation='tanh') # action is bounded by +/- 1\n",
    "\n",
    "    def call(self, state):\n",
    "        temp1  = self.fc1(state)\n",
    "        temp2  = self.fc2(temp1)\n",
    "        action = self.mu(temp2)\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
